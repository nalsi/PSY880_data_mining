---
title: "PSY880 Homework 2"
author: "Kai Li"
date: "1/25/2017"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, warning = T)

library(pander)

```

## Question 1

#### (a)

If the Bayes decision boundary is linear, LDA should work better than QDA in both cases, because of LDA is linear as well. However, QDA's high flexibility could compensate the gap in the training but not test set.

#### (b)

If the Bayes decision boundary is non-linear, QDA should work better than LDA in both cases.

#### (c)

In general, if the sample size increases, QDA should have better performance as compared to LDA. It is because QDA needs to have enough sample size so that the variance can be accurately estimated.

#### (d)

It's false. It is true that QDA has higher flexibility than LDA. However, the reason stated in the question is irrelevant. It is the nature of data (linearity, variance and normality) that determines the performance of model.

## Question 2

#### Logistic regression

Following are the result of using a randomly selected 300 towns as training set, and the rest (206 towns) as test set.

``` {r}

library(MASS)

# Read data
data <- Boston

# Set categorical variable "crim.1" instead of "crim"
crim.1 <- rep(0, nrow(data))
crim.1[data$crim > median(data$crim)] <- 1
data <- cbind(data, crim.1)

# Split the dataset into train and test sets
set.seed(5)
train <- sample(506, 300)
test <- c(1:506)[-train]
train.set <- data[train,]
test.set <- data[-train,]

# Model with all the other variables as independent variables
model.1.1 <- glm(crim.1 ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat + medv, 
                 data = data,
                 family = binomial,
                 subset = train)
summary(model.1.1)

``` 

As shown in the summary of this model, one of the problems is that a large number of the variables are not significant predictors of the dependent variable. And at the same time, nox is too strong a predictor as compared with other variables. But the prediction is still done following this model.

``` {R}

# Prediction based on the model
# Train set prediction

probs.1.1 <- predict(model.1.1, train.set, type = "response")
prediction.1.1 <- rep(0, length(probs.1.1))
prediction.1.1[probs.1.1 > 0.5] <- 1
table(prediction.1.1, train.set$crim.1)
accuracy_rate_train.1.1 <- round(mean(prediction.1.1 == train.set$crim.1), digits = 4) * 100

# Test set prediction

probs.1.2 <- predict(model.1.1, test.set, type = "response")
prediction.1.2 <- rep(0, length(probs.1.2))
prediction.1.2[probs.1.2 > 0.5] <- 1

# Results of the first model
table(prediction.1.2, test.set$crim.1)
accuracy_rate_test.1.2 <- round(mean(prediction.1.2 == test.set$crim.1), digits = 4) * 100

```

The accuracy rate of this model is `r accuracy_rate_test.1.2`%. As a comparison, the accuracy rate for the train set is `r accuracy_rate_train.1.1`%. And as 

But to address the issues mentioned above, "stepAIC" function within the package "MASS" is used to determine the best glm model (in terms of the lowest AIC value), which is "crim.1 ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat". So this model is used to build the second glm model.

``` {R eval = F}

# Results too long, so not run
stepAIC(model.1.1)

```

``` {R}

# Second glm model
model.1.2 <- glm(crim.1 ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat, 
                 data = data,
                 family = binomial,
                 subset = train)

```

``` {R}

# Prediction based on the second model
probs.1.3 <- predict(model.1.2, test.set, type = "response")
prediction.1.3 <- rep(0, length(probs.1.3))
prediction.1.3[probs.1.3 > 0.5] <- 1

# Results of the second model
table(prediction.1.3, test.set$crim.1)

accuracy_rate_test.1.3 <- round(mean(prediction.1.3 == test.set$crim.1), digits = 4) * 100

```

It seems that the new model resulted a lower accuracy rate (`r accuracy_rate_test.1.3`%) on the test set, even though the model per se is a better one.

#### LDA

``` {R}

lda.model.1 <- lda(crim.1 ~ zn + indus + chas + nox + rm + age + dis + rad + tax + ptratio + black + lstat + medv,
                 data=data,
                 subset=train)

lda.pred.1.1 = predict(lda.model.1, test.set)
lda.class.1.1 = lda.pred.1.1$class

accuracy_rate_lda.1.1 <- round(mean(lda.class.1.1 == test.set$crim.1), digits = 4) * 100

```

Both models that were examined in the last section are also examined in the LDA test. For the fuller model, below is the summary of the prediction. The overall accuracy rate is `r accuracy_rate_lda.1.1`%.

``` {R}

table(lda.class.1.1, test.set$crim.1)

```

``` {R}

lda.model.2 <- lda(crim.1 ~ zn + chas + nox + rm + dis + rad + tax + ptratio + lstat,
                 data=data,
                 subset=train)

lda.pred.1.2 = predict(lda.model.2, test.set)
lda.class.1.2 = lda.pred.1.2$class

accuracy_rate_lda.1.2 <- round(mean(lda.class.1.2 == test.set$crim.1), digits = 4) * 100

```

Below is the summary of the prediction based on the second model. The accuracy rate is `r accuracy_rate_lda.1.2`%. Similar to logit model, the accuracy rate based on the second model is also lower than that based on the first.

``` {R}

table(lda.class.1.2, test.set$crim.1)

```

## KNN

The same two models are also tested using KNN model. 

``` {R}

library(class)
attach(Boston)

# Create the predictors for the training set
train.set.pred = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[train,]
test.set.pred = cbind(zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, lstat, medv)[test,]
train.set.crim = crim.1[train]
test.set.crim = crim.1[test]

# Prediction based on k = 1
set.seed(2)
knn.pred.1 = knn(train.set.pred, test.set.pred, train.set.crim, k = 1)
accuracy_rate_knn.1.1 <- round(mean(knn.pred.1 == test.set.crim), digits = 4) * 100

```

Below is the summary of the result using the model or the fuller model. When k is set as 1, the overall accuracy rate for this model is `r accuracy_rate_knn.1.1`%.

``` {R}

table(knn.pred.1, test.set.crim)

```

``` {R}

#with a different K

set.seed(2)
knn.pred.2 = knn(train.set.pred, test.set.pred, train.set.crim, k = 3)
table(knn.pred.2, test.set.crim)
accuracy_rate_knn.1.2 <- round(mean(knn.pred.2 == test.set.crim), digits = 4) * 100

```

And when K is changed to 3, the accuracy rate is `r accuracy_rate_knn.1.2`%. And the confusion table is plotted below.

``` {R}

table(knn.pred.2, test.set.crim)

```

Below is the summary of accuracies using the second model when k is 1 and 3 respectively.

``` {R}

train.set.pred.2 = cbind(zn, chas, nox, rm, dis, rad, tax, ptratio, lstat)[train,]
test.set.pred.2 = cbind(zn, chas, nox, rm, dis, rad, tax, ptratio, lstat)[test,]
train.set.crim = crim.1[train]
test.set.crim = crim.1[test]

# Prediction based on k = 1
knn.pred.2.1 = knn(train.set.pred.2, test.set.pred.2, train.set.crim, k = 1)
accuracy_rate_knn.2.1 <- round(mean(knn.pred.2.1 == test.set.crim), digits = 4) * 100

# Prediction based on k = 3
knn.pred.2.2 = knn(train.set.pred.2, test.set.pred.2, train.set.crim, k = 3)
accuracy_rate_knn.2.2 <- round(mean(knn.pred.2.2 == test.set.crim), digits = 4) * 100

table <- data.frame(K = c(1,3),
                    Accuracy = c(accuracy_rate_knn.2.1, accuracy_rate_knn.2.2))
pander(table)

```
